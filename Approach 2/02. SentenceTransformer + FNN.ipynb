{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings( 'ignore' )\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from lightning_fabric import Fabric\n",
    "\n",
    "\n",
    "from utils.config import Parameters\n",
    "from utils.dataset import Dataset\n",
    "from utils.performance_evaluation import performance_evaluation\n",
    "from utils.utils import format_time\n",
    "from utils.logger import init_logger \n",
    "from utils.early_stopping import EarlyStopping\n",
    "from utils.Classifier import Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "# When running on the CuDNN backend, two further options must be set\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "# Set a fixed value for the hash seed\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "\n",
    "\n",
    "# Setup Fabric\n",
    "fabric = Fabric(accelerator=\"auto\", devices=\"auto\", precision=\"bf16-mixed\", strategy=\"auto\")\n",
    "fabric.launch()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get parameters\n",
    "args = Parameters()\n",
    "\n",
    "# Project directory\n",
    "if (not os.path.exists(args.output_dir)):\n",
    "    os.mkdir(args.output_dir)\n",
    "\n",
    "# Create logger\n",
    "logger = init_logger(log_file = 'logs.log') "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(args.dataset_path, sep='&')\n",
    "df['label'] = df['label'].astype('float')\n",
    "# Split Train/Test datasets\n",
    "df_train, df_test = train_test_split(df, test_size=args.test_size, shuffle=True, stratify=df['label'])\n",
    "# Create validation dataset\n",
    "df_train, df_valid = train_test_split(df_train, test_size=args.valid_size, shuffle=True, stratify=df_train['label'])\n",
    "\n",
    "# Reset indices\n",
    "df_train = df_train.reset_index().drop(['index'], axis=1)\n",
    "df_valid = df_valid.reset_index().drop(['index'], axis=1) \n",
    "df_test = df_test.reset_index().drop(['index'], axis=1)\n",
    "\n",
    "# Class Visualization\n",
    "plt.figure(figsize = (6,2))\n",
    "plt.hist(df_train['label']);\n",
    "plt.hist(df_test['label']);\n",
    "plt.hist(df_valid['label']);\n",
    "\n",
    "plt.legend(['Training', 'Validation', 'Testing'], frameon=False);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "train_dataset = Dataset(df_train)\n",
    "# Create data loader\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.train_batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "valid_dataset = Dataset(df_valid)\n",
    "# Create data loader\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=args.train_batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Create dataset\n",
    "test_dataset = Dataset(df_test)\n",
    "# Create data loader\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.test_batch_size, shuffle=False)\n",
    "\n",
    "logger.info('Dataloader were created')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.Classifier import Classifier\n",
    "\n",
    "model = Classifier(args=args).to(args.device);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "\n",
    "\n",
    "optimizer_parameters = [\n",
    "    {\n",
    "        \"params\": [\n",
    "            p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": args.weight_decay,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [\n",
    "            p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "\n",
    "# Setup optimizer\n",
    "if args.optimizer == 'AdamW':\n",
    "    optimizer = torch.optim.AdamW(optimizer_parameters, lr=args.learning_rate)\n",
    "elif args.optimizer == 'RMSprop':\n",
    "    optimizer = torch.optim.RMSprop(optimizer_parameters, lr=args.learning_rate)  \n",
    "elif args.optimizer == 'Adam':\n",
    "    optimizer = torch.optim.Adam(optimizer_parameters, lr=args.learning_rate)\n",
    "elif args.optimizer == 'SGD':\n",
    "    optimizer = torch.optim.SGD(optimizer_parameters, lr=args.learning_rate, momentum=args.momentum)\n",
    "    \n",
    "# Setup scheduler\n",
    "num_train_steps = int(train_loader.__len__() / args.train_batch_size * args.epochs)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer=optimizer, \n",
    "                                            num_warmup_steps=0, \n",
    "                                            num_training_steps=num_train_steps)\n",
    "# from utils.scheduler import LRScheduler\n",
    "# scheduler = LRScheduler(optimizer = optimizer, \n",
    "#                         patience  = 10, \n",
    "#                         min_lr    = 1e-8, \n",
    "#                         factor    = 0.5, \n",
    "#                         verbose   = True)\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(patience  = 20,\n",
    "                               min_delta = 0)\n",
    "\n",
    "logger.info('Training parameters were setup')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert model, loaders & optimizer to fabric setup\n",
    "model, optimizer = fabric.setup(model, optimizer)\n",
    "\n",
    "train_loader = fabric.setup_dataloaders(train_loader)\n",
    "valid_loader = fabric.setup_dataloaders(valid_loader)\n",
    "test_loader = fabric.setup_dataloaders(test_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_AUC = 0\n",
    "history = {'train_loss': [], 'valid_loss': [], \n",
    "           'train_accuracy': [], 'valid_accuracy': [], \n",
    "           'train_AUC': [], 'valid_AUC': []}\n",
    "\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Activate training mode\n",
    "    model.train()\n",
    "    \n",
    "    # setup loop with TQDM and dataloader\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    # setup epoch's metrics\n",
    "    metrics = {'losses': [], 'accuracy': [], 'AUC': []}\n",
    "    # initialize calculated gradients (from prev step)\n",
    "    optimizer.zero_grad()\n",
    "    for step, batch in enumerate(loop):\n",
    "        # pull all tensor batches required for training\n",
    "        text = batch['text']\n",
    "        labels = batch['labels'].to(args.device)\n",
    "        # Get loss and predictions\n",
    "        loss, predictions = model(text=text, labels=labels)      \n",
    "        # Calculate performance metrics\n",
    "        accuracy, AUC, _ = performance_evaluation(labels, predictions)\n",
    "        # extract loss - normalized\n",
    "        loss = loss / args.number_accumulated_gradients      \n",
    "        # Backpropagate errors  \n",
    "        fabric.backward(loss)\n",
    "\n",
    "        if (step+1) % args.number_accumulated_gradients == 0 or (step+1) % len(train_loader) == 0: \n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "            # Update scheduler\n",
    "            scheduler.step()\n",
    "            # Reset gradients tensors\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Add loss/accuracy/AUC\n",
    "        metrics['losses'].append(loss.item())\n",
    "        metrics['accuracy'].append(accuracy)\n",
    "        metrics['AUC'].append(AUC)\n",
    "\n",
    "\n",
    "        # add stuff to progress bar in the end\n",
    "        loop.set_description(f\"Epoch [{epoch+1}/{args.epochs}]\")\n",
    "        loop.set_postfix(loss=np.mean(metrics['losses']), accuracy=f\"{np.mean(metrics['accuracy']):.2f}%\", AUC=np.mean(metrics['AUC']))\n",
    "\n",
    "    # Calculate test loss/accuracy/AUC\n",
    "    train_loss = np.mean(metrics['losses'])\n",
    "    train_accuracy = np.mean(metrics['accuracy'])\n",
    "    train_AUC= np.mean(metrics['AUC'])\n",
    "\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    # setup loop with TQDM and dataloader\n",
    "    loop = tqdm(valid_loader, leave=True)\n",
    "    # setup epoch's metrics\n",
    "    metrics = {'losses': [], 'accuracy': [], 'AUC': [], 'CM': None}\n",
    "    for step, batch in enumerate(loop):\n",
    "\n",
    "        # pull all tensor batches required for training\n",
    "        text = batch['text']\n",
    "        labels = batch['labels'].to(args.device)\n",
    "        # Get loss & predictions\n",
    "        loss, predictions = model(text=text, labels=labels)\n",
    "        \n",
    "        # Calculate performance metrics\n",
    "        accuracy, AUC, CM = performance_evaluation(labels, predictions)\n",
    "        # Add loss/accuracy/AUC\n",
    "        metrics['losses'].append(loss.item())\n",
    "        metrics['accuracy'].append(accuracy)\n",
    "        metrics['AUC'].append(AUC)\n",
    "        if metrics['CM'] is None: metrics['CM'] = CM \n",
    "        else: metrics['CM'] += CM\n",
    "\n",
    "        loop.set_description(\"Validation\")    \n",
    "        loop.set_postfix(loss=np.mean(metrics['losses']), accuracy=f\"{np.mean(metrics['accuracy']):.2f}%\", AUC=np.mean(metrics['AUC']))\n",
    "\n",
    "    # Calculate test loss/accuracy/AUC\n",
    "    valid_loss = np.mean(metrics['losses'])\n",
    "    valid_accuracy = np.mean(metrics['accuracy'])\n",
    "    valid_AUC= np.mean(metrics['AUC'])\n",
    "    # Elapsed time per epoch\n",
    "    elapsed = format_time(time.time() - t0)\n",
    "\n",
    "\n",
    "    # Store performance\n",
    "    history['train_loss'].append(train_loss)    \n",
    "    history['valid_loss'].append(valid_loss)\n",
    "    history['train_accuracy'].append(train_accuracy)    \n",
    "    history['valid_accuracy'].append(valid_accuracy)\n",
    "    history['train_AUC'].append(train_AUC)    \n",
    "    history['valid_AUC'].append(valid_AUC) \n",
    "\n",
    "    \n",
    "    # Update best model\n",
    "    if valid_AUC > best_AUC:\n",
    "        torch.save(model, os.path.join(args.output_dir, \"model.pt\"))\n",
    "        torch.save(model.state_dict(), os.path.join(args.output_dir, \"pytorch_model.bin\"))\n",
    "        best_AUC = valid_AUC    \n",
    "        \n",
    "    # # Learning rate scheduler\n",
    "    # scheduler(valid_loss)\n",
    "\n",
    "    # Early Stopping\n",
    "    if early_stopping(valid_loss): break    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame.from_dict(history)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 3))\n",
    "df_results[['train_accuracy','valid_accuracy']].plot(ax=ax[0], marker='o')\n",
    "df_results[['train_loss','valid_loss']].plot(ax=ax[1], marker='o')\n",
    "ax[0].legend(frameon=False, fontsize=12);\n",
    "ax[1].legend(frameon=False, fontsize=12);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation of Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load optimized model\n",
    "model = torch.load(f\"{args.output_dir}/model.pt\")\n",
    "model.eval();\n",
    "\n",
    "print('[INFO] Model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    # setup loop with TQDM and dataloader\n",
    "    loop = tqdm(test_loader, leave=True)\n",
    "    # setup epoch's metrics\n",
    "    metrics = {'losses': [], 'accuracy': [], 'AUC': [], 'CM': 0}\n",
    "    for step, batch in enumerate(loop):\n",
    "        # pull all tensor batches required for training\n",
    "        text = batch['text']\n",
    "        labels = batch['labels'].to(args.device)\n",
    "        # Get loss & predictions\n",
    "        loss, predictions = model(text=text, labels=labels)\n",
    "        # Calculate performance metrics\n",
    "        accuracy, AUC, CM = performance_evaluation(labels, predictions)\n",
    "        \n",
    "        # Add loss/accuracy/AUC\n",
    "        metrics['losses'].append(loss.item())\n",
    "        metrics['accuracy'].append(accuracy)\n",
    "        metrics['AUC'].append(AUC)\n",
    "        if metrics['CM'] is None: metrics['CM'] = CM \n",
    "        else: metrics['CM'] += CM\n",
    "        \n",
    "        loop.set_description(\"Validation\")    \n",
    "        loop.set_postfix(loss=np.mean(metrics['losses']), accuracy=f\"{np.mean(metrics['accuracy']):.2f}%\", AUC=np.mean(metrics['AUC']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loss: {np.mean(metrics['losses']):.3f}\")\n",
    "print(f\"Accuracy: {np.mean(metrics['accuracy']):.2f}%\")\n",
    "print(f\"AUC: {np.mean(metrics['AUC']):.3f}\")\n",
    "print(metrics['CM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# # Serialize JSON by converting all objects to strings\n",
    "# d = vars(args).copy()\n",
    "# d = {x:str(d[x]) for x in d.keys()}\n",
    "\n",
    "# # Store parameters\n",
    "# with open(args.output_dir + '/parameters.json', 'w') as fp:\n",
    "#     json.dump(d, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
